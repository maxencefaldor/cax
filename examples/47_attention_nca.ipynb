{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Cellular Automata [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maxencefaldor/cax/blob/main/examples/47_attention_nca.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need Python 3.11 or later, and a working JAX installation. For example, you can install JAX with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"jax[cuda]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, install CAX from PyPi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"cax[examples]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import mediapy\n",
    "import optax\n",
    "import torchvision\n",
    "from flax import nnx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cax.core import ComplexSystem, Input, State\n",
    "from cax.core.perceive import Perceive, Perception\n",
    "from cax.core.update import ResidualUpdate\n",
    "from cax.nn.pool import Pool\n",
    "from cax.utils import clip_and_uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "spatial_dims = (28, 28)\n",
    "channel_size = 32\n",
    "perception_size = 64\n",
    "num_heads = 4\n",
    "hidden_size = 128\n",
    "proj_size = 32\n",
    "cell_dropout_rate = 0.5\n",
    "\n",
    "num_steps = 32\n",
    "pool_size = 1_024\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "mask_ratio = 0.5\n",
    "\n",
    "key = jax.random.key(seed)\n",
    "rngs = nnx.Rngs(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "ds_train = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "ds_test = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "# Convert to jax.Array\n",
    "y_train = jnp.array([y.resize(spatial_dims) for y, _ in ds_train])[..., None] / 255\n",
    "y_test = jnp.array([y.resize(spatial_dims) for y, _ in ds_test])[..., None] / 255\n",
    "\n",
    "# Visualize\n",
    "mediapy.show_images(y_train[:8], width=128, height=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTPerceive(Perceive):\n",
    "\t\"\"\"Vision Transformer Perceive class.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tchannel_size: int,\n",
    "\t\tperception_size: int,\n",
    "\t\t*,\n",
    "\t\tnum_heads: int,\n",
    "\t\thidden_size: int,\n",
    "\t\tproj_size: int,\n",
    "\t\tmax_position_size: int,\n",
    "\t\tposition_embed_features: int = 4,\n",
    "\t\trngs: nnx.Rngs,\n",
    "\t):\n",
    "\t\t\"\"\"Initialize ViT Perceive.\"\"\"\n",
    "\t\tself.linear = nnx.Linear(in_features=channel_size, out_features=proj_size, rngs=rngs)\n",
    "\n",
    "\t\tself.position_embed_features = position_embed_features\n",
    "\t\tself.position_embed = nnx.Embed(\n",
    "\t\t\tnum_embeddings=max_position_size,\n",
    "\t\t\tfeatures=position_embed_features,\n",
    "\t\t\trngs=rngs,\n",
    "\t\t)\n",
    "\n",
    "\t\tself.attention = nnx.MultiHeadAttention(\n",
    "\t\t\tnum_heads=num_heads,\n",
    "\t\t\tin_features=proj_size + 2 * position_embed_features,\n",
    "\t\t\tqkv_features=hidden_size,\n",
    "\t\t\tout_features=perception_size,\n",
    "\t\t\tdecode=False,\n",
    "\t\t\trngs=rngs,\n",
    "\t\t)\n",
    "\n",
    "\tdef __call__(self, state: State) -> Perception:\n",
    "\t\t\"\"\"Apply perception to the input state.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tstate: State of the cellular automaton.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tThe perceived state after applying convolutional layers.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t# Linear projection of state into tokens\n",
    "\t\tstate = self.linear(state)\n",
    "\n",
    "\t\t# Concatenate position embed\n",
    "\t\tposition_embed_h = self.position_embed(jnp.arange(state.shape[-3]))\n",
    "\t\tposition_embed_w = self.position_embed(jnp.arange(state.shape[-2]))\n",
    "\t\tposition_embed = jnp.concatenate(\n",
    "\t\t\t[\n",
    "\t\t\t\tjnp.repeat(position_embed_h[:, None, :], state.shape[-2], axis=1),\n",
    "\t\t\t\tjnp.repeat(position_embed_w[None, :, :], state.shape[-3], axis=0),\n",
    "\t\t\t],\n",
    "\t\t\taxis=-1,\n",
    "\t\t)\n",
    "\t\ttokens = jnp.concatenate([state, position_embed], axis=-1)\n",
    "\n",
    "\t\t# Get mask for localized attention\n",
    "\t\tmask = self.get_mask(tokens)\n",
    "\n",
    "\t\t# Flatten grid into a sequence of tokens\n",
    "\t\ttokens = jnp.reshape(tokens, tokens.shape[:-3] + (-1, tokens.shape[-1]))\n",
    "\n",
    "\t\t# Apply localized attention\n",
    "\t\tperception = self.attention(tokens, mask=mask)\n",
    "\t\tperception = jnp.reshape(\n",
    "\t\t\tperception,\n",
    "\t\t\tperception.shape[:-2] + (state.shape[-3], state.shape[-2], perception.shape[-1]),\n",
    "\t\t)\n",
    "\n",
    "\t\treturn perception\n",
    "\n",
    "\tdef get_mask(self, tokens: jax.Array) -> jax.Array:\n",
    "\t\t\"\"\"Get mask for localized attention using Moore neighborhood.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\ttokens: Input tokens with shape [..., H, W, C]\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tBoolean mask with shape [..., H*W, H*W] where True values indicate\n",
    "\t\t\tallowed attention connections between tokens.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\th, w = tokens.shape[-3], tokens.shape[-2]\n",
    "\n",
    "\t\t# Create position indices\n",
    "\t\trow_idx = jnp.arange(h)[:, None, None, None]  # [H, 1, 1, 1]\n",
    "\t\tcol_idx = jnp.arange(w)[None, :, None, None]  # [1, W, 1, 1]\n",
    "\n",
    "\t\t# Broadcast to full grid\n",
    "\t\trow1 = jnp.broadcast_to(row_idx, (h, w, h, w))  # Source positions\n",
    "\t\tcol1 = jnp.broadcast_to(col_idx, (h, w, h, w))\n",
    "\t\trow2 = jnp.broadcast_to(row_idx.transpose((2, 3, 0, 1)), (h, w, h, w))  # Target positions\n",
    "\t\tcol2 = jnp.broadcast_to(col_idx.transpose((2, 3, 0, 1)), (h, w, h, w))\n",
    "\n",
    "\t\t# Calculate Manhattan distance between all positions\n",
    "\t\trow_dist = jnp.abs(row1 - row2)\n",
    "\t\tcol_dist = jnp.abs(col1 - col2)\n",
    "\n",
    "\t\t# Create mask where True allows attention (distance <= 1 in both dimensions)\n",
    "\t\tmask = (row_dist <= 1) & (col_dist <= 1)\n",
    "\n",
    "\t\t# Reshape to attention matrix shape\n",
    "\t\tmask = jnp.reshape(mask, (h * w, h * w))\n",
    "\n",
    "\t\treturn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTNCA(ComplexSystem):\n",
    "\t\"\"\"ViT Neural Cellular Automata class.\"\"\"\n",
    "\n",
    "\tdef __init__(self, *, rngs: nnx.Rngs):\n",
    "\t\t\"\"\"Initialize ViT NCA.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\trngs: rng key.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\tself.perceive = ViTPerceive(\n",
    "\t\t\tchannel_size=channel_size,\n",
    "\t\t\tperception_size=perception_size,\n",
    "\t\t\tnum_heads=num_heads,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tproj_size=proj_size,\n",
    "\t\t\tmax_position_size=max(spatial_dims),\n",
    "\t\t\trngs=rngs,\n",
    "\t\t)\n",
    "\t\tself.update = ResidualUpdate(\n",
    "\t\t\tnum_spatial_dims=2,\n",
    "\t\t\tchannel_size=channel_size,\n",
    "\t\t\tperception_size=perception_size,\n",
    "\t\t\thidden_layer_sizes=(hidden_size,),\n",
    "\t\t\tcell_dropout_rate=cell_dropout_rate,\n",
    "\t\t\tzeros_init=True,\n",
    "\t\t\trngs=rngs,\n",
    "\t\t)\n",
    "\n",
    "\tdef _step(self, state: State, input: Input | None = None, *, sow: bool = False) -> State:\n",
    "\t\tperception = self.perceive(state)\n",
    "\t\tnext_state = self.update(state, perception, input)\n",
    "\n",
    "\t\tif sow:\n",
    "\t\t\tself.sow(nnx.Intermediate, \"state\", next_state)\n",
    "\n",
    "\t\treturn next_state\n",
    "\n",
    "\t@nnx.jit\n",
    "\tdef render(self, state):\n",
    "\t\t\"\"\"Render state to RGB.\"\"\"\n",
    "\t\tgray = state[..., -1:]\n",
    "\t\trgb = jnp.repeat(gray, 3, axis=-1)\n",
    "\n",
    "\t\t# Clip values to valid range and convert to uint8\n",
    "\t\treturn clip_and_uint8(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ViTNCA(rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = nnx.state(cs, nnx.Param)\n",
    "print(\"Number of params:\", sum(x.size for x in jax.tree.leaves(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_state(key):\n",
    "\t\"\"\"Sample a state with a randomly masked image.\"\"\"\n",
    "\t# Sample image from dataset\n",
    "\ty_idx = jax.random.choice(key, y_train.shape[0])\n",
    "\ty = y_train[y_idx]\n",
    "\ty_channel_size = y.shape[-1]\n",
    "\n",
    "\t# Init state with zeros\n",
    "\tstate = jnp.zeros(spatial_dims + (channel_size,))\n",
    "\n",
    "\t# Mask pixels randomly\n",
    "\tmask = jax.random.bernoulli(key, 1 - mask_ratio, shape=spatial_dims)\n",
    "\tmask = jnp.expand_dims(mask, axis=-1)\n",
    "\ty *= mask\n",
    "\n",
    "\t# Set state\n",
    "\tstate = state.at[..., -y_channel_size:].set(y)\n",
    "\n",
    "\treturn state, y_idx\n",
    "\n",
    "\n",
    "def sample_state_test(key):\n",
    "\t\"\"\"Sample a state with a randomly masked image.\"\"\"\n",
    "\t# Sample image from dataset\n",
    "\ty_idx = jax.random.choice(key, y_test.shape[0])\n",
    "\ty = y_test[y_idx]\n",
    "\ty_channel_size = y.shape[-1]\n",
    "\n",
    "\t# Init state with zeros\n",
    "\tstate = jnp.zeros(spatial_dims + (channel_size,))\n",
    "\n",
    "\t# Mask pixels randomly\n",
    "\tmask = jax.random.bernoulli(key, 1 - mask_ratio, shape=spatial_dims)\n",
    "\tmask = jnp.expand_dims(mask, axis=-1)\n",
    "\ty *= mask\n",
    "\n",
    "\t# Set state\n",
    "\tstate = state.at[..., -y_channel_size:].set(y)\n",
    "\n",
    "\treturn state, y_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "keys = jax.random.split(subkey, pool_size)\n",
    "state, y_idx = jax.vmap(sample_state)(keys)\n",
    "\n",
    "pool = Pool.create({\"state\": state, \"y_idx\": y_idx})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = optax.linear_schedule(\n",
    "\tinit_value=learning_rate, end_value=0.1 * learning_rate, transition_steps=4_096\n",
    ")\n",
    "\n",
    "optimizer = optax.chain(\n",
    "\toptax.clip_by_global_norm(1.0),\n",
    "\toptax.adam(learning_rate=lr_sched),\n",
    ")\n",
    "\n",
    "optimizer = nnx.Optimizer(cs, optimizer, wrt=nnx.Param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(state, y):\n",
    "\t\"\"\"Mean Squared Error.\"\"\"\n",
    "\treturn jnp.mean(jnp.square(state[..., -1:] - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def loss_fn(cs, state, y):\n",
    "\t\"\"\"Loss function.\"\"\"\n",
    "\tstate_axes = nnx.StateAxes({nnx.RngState: 0, nnx.Intermediate: 0, ...: None})\n",
    "\tstate = nnx.split_rngs(splits=batch_size)(\n",
    "\t\tnnx.vmap(\n",
    "\t\t\tlambda cs, state: cs(state, num_steps=num_steps),\n",
    "\t\t\tin_axes=(state_axes, 0),\n",
    "\t\t)\n",
    "\t)(cs, state)\n",
    "\n",
    "\tloss = mse(state, y)\n",
    "\treturn loss, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(cs, optimizer, pool, key):\n",
    "\t\"\"\"Train step.\"\"\"\n",
    "\tsample_key, sample_state_key = jax.random.split(key)\n",
    "\n",
    "\t# Sample from pool\n",
    "\tpool_idx, batch = pool.sample(sample_key, batch_size=batch_size)\n",
    "\tcurrent_state = batch[\"state\"]\n",
    "\tcurrent_y_idx = batch[\"y_idx\"]\n",
    "\tcurrent_y = y_train[current_y_idx]\n",
    "\n",
    "\t# Sort by descending loss\n",
    "\tsort_idx = jnp.argsort(jax.vmap(mse)(current_state, current_y), descending=True)\n",
    "\tpool_idx = pool_idx[sort_idx]\n",
    "\tcurrent_state = current_state[sort_idx]\n",
    "\tcurrent_y_idx = current_y_idx[sort_idx]\n",
    "\n",
    "\t# Sample a new image to replace the worst\n",
    "\tnew_state, new_y_idx = sample_state(sample_state_key)\n",
    "\tcurrent_state = current_state.at[0].set(new_state)\n",
    "\tcurrent_y_idx = current_y_idx.at[0].set(new_y_idx)\n",
    "\tcurrent_y = y_train[current_y_idx]\n",
    "\n",
    "\t(loss, current_state), grad = nnx.value_and_grad(loss_fn, has_aux=True)(\n",
    "\t\tcs, current_state, current_y\n",
    "\t)\n",
    "\toptimizer.update(cs, grad)\n",
    "\n",
    "\tpool = pool.update(pool_idx, {\"state\": current_state, \"y_idx\": current_y_idx})\n",
    "\treturn loss, pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = 8_196\n",
    "print_interval = 128\n",
    "\n",
    "pbar = tqdm(range(num_train_steps), desc=\"Training\", unit=\"train_step\")\n",
    "losses = []\n",
    "for i in pbar:\n",
    "\tkey, subkey = jax.random.split(key)\n",
    "\tloss, pool = train_step(cs, optimizer, pool, subkey)\n",
    "\tlosses.append(loss)\n",
    "\n",
    "\tif i % print_interval == 0 or i == num_train_steps - 1:\n",
    "\t\tavg_loss = sum(losses[-print_interval:]) / len(losses[-print_interval:])\n",
    "\t\tpbar.set_postfix({\"Average Loss\": f\"{avg_loss:.3e}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 8\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, num_examples)\n",
    "state_init, y_idx = jax.vmap(sample_state_test)(keys)\n",
    "\n",
    "state_axes = nnx.StateAxes({nnx.RngState: 0, nnx.Intermediate: 0, ...: None})\n",
    "state_final = nnx.split_rngs(splits=num_examples)(\n",
    "\tnnx.vmap(\n",
    "\t\tlambda cs, state: cs(state, num_steps=2 * num_steps, sow=True),\n",
    "\t\tin_axes=(state_axes, 0),\n",
    "\t)\n",
    ")(cs, state_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediates = nnx.pop(cs, nnx.Intermediate)\n",
    "states = intermediates.state.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = jnp.concatenate([state_init[:, None], states], axis=1)\n",
    "frame_init = nnx.vmap(\n",
    "\tlambda cs, state: cs.render(state),\n",
    "\tin_axes=(None, 0),\n",
    ")(cs, state_init)\n",
    "\n",
    "frames = nnx.vmap(\n",
    "\tlambda cs, state: cs.render(state),\n",
    "\tin_axes=(None, 0),\n",
    ")(cs, states)\n",
    "\n",
    "mediapy.show_images(y_test[y_idx], width=128, height=128)\n",
    "mediapy.show_images(frame_init, width=128, height=128)\n",
    "mediapy.show_videos(frames, width=128, height=128, codec=\"gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
